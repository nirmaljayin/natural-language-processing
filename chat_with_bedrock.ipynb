{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e09801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import boto3\n",
    "import botocore.config\n",
    "import pandas as pd\n",
    "from langchain_aws.chat_models import ChatBedrock, ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  # noqa: F401\n",
    "from langchain_core.prompts import (  # noqa: F401\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from pandas.io.formats import excel\n",
    "\n",
    "excel.ExcelFormatter.header_style = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b699949",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session = boto3.Session()\n",
    "boto3_config = botocore.config.Config(retries={\"max_attempts\": 0}, read_timeout=1200)\n",
    "bedrock_runtime = boto3_session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    "    config=boto3_config,\n",
    "    endpoint_url=\"https://vpce-083a462e0de27372a-na8wleza.bedrock-runtime.us-east-1.vpce.amazonaws.com\",\n",
    ")\n",
    "\n",
    "chat_model = ChatBedrockConverse(\n",
    "    client=bedrock_runtime,\n",
    "    model=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    # model=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "chat_model = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    # model=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "data = pd.read_csv(\"./data/clinical_trial.csv\")\n",
    "\n",
    "code_generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"You are an expert Data Scientist with extensive experience in python programming \"\n",
    "                        \"and specializing in Deep Reinforcement Learning\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"You have to follow the instructions given by the student and help him in creating \"\n",
    "                        \"the code for solving the assignment. You have to only generate the code for completing the task. \"\n",
    "                        \"The data might be\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"STRICTLY provide code only in python language. \"\n",
    "                        \"STRICTLY return ONLY the python code in markdown format. \"\n",
    "                        \"Add comments in the code for understanding purpose\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\"type\": \"default\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"human\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Assignment title: Adaptive Treatment Selection with Multi-Armed Bandits\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Scenario: A pharmaceutical company is conducting clinical trials to evaluate the \"\n",
    "                        \"effectiveness of three antiretroviral drug combinations for treating HIV-positive patients. \"\n",
    "                        \"Due to the ethical and cost constraints of clinical trials, it is critical to identify the \"\n",
    "                        \"most effective treatment regimen using the least number of patients. Each treatment \"\n",
    "                        \"(or “arm”) can lead to different outcomes depending on patient responses. \"\n",
    "                        \"The effectiveness of each \"\n",
    "                        \"treatment is evaluated using a reward function derived from the improvement in \"\n",
    "                        \"patients' immune system markers and survival status.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Objective: You are provided with a clinical dataset where each record corresponds to a \"\n",
    "                        \"patient, including the treatment they received and the resulting health outcomes. \"\n",
    "                        \"Your task is to simulate a clinical trial environment using various MAB strategies \"\n",
    "                        \"to sequentially recommend treatments and observe outcomes. The objective is to maximize \"\n",
    "                        \"the overall success rate across trials by identifying and \"\n",
    "                        \"favouring the most effective treatment.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Dataset Description: The dataset containing the following fields:\n",
    "\n",
    "Age (age): Patient's age in years at baseline.\n",
    "Weight (wtkg): Continuous feature representing weight in kilograms at baseline.\n",
    "Gender (gender): Binary indicator of gender (0 = Female, 1 = Male).\n",
    "CD4 Counts (cd40, cd420): Integer values representing CD4 counts at baseline and 20+/-5 weeks.\n",
    "Treatment Indicator (trt): Categorical feature indicating the type of treatment received (0 = ZDV only, 1 = ZDV + ddI, 2 = ZDV + Zal, 3 = ddI only).\n",
    "Censoring Indicator (label): Binary indicator (1 = failure, 0 = censoring) denoting patient status.\n",
    "\n",
    "Data is csv and given below.\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (f\"{data.to_csv(index=False)}\"),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Environment Details:\n",
    "Arms (Actions): The treatment types\n",
    "Arm 0: ZDV only\n",
    "Arm 1: ZDV + ddI\n",
    "Arm 2: ZDV + Zal\n",
    "Arm 3: ddI only\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Reward Function:\n",
    "Reward r is defined as:\n",
    "r = 1, if (label == 0) and (cd420 > cd40)\n",
    "r = 0, otherwise\n",
    "This reward represents a successful treatment outcome as an increase in CD4 count and survival.\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Assumptions: Number of Iterations: Simulate at least 1000 trials (iterations). \"\n",
    "                        \"In each iteration, simulate one patient trial using one of the bandit policies.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Requirements and Deliverables:\n",
    "1. Load the clinical treatment dataset. (0.5 Mark)\n",
    "2. Define the bandit environment with treatment arms and compute the binary reward using CD4 count improvement and patient survival. (0.5 Mark)\n",
    "3. Implement the Random policy for treatment selection. Run the simulation for at least 1000 iterations and print the treatment selected and reward at each iteration. (0.5 Mark)\n",
    "4. Implement the Greedy policy that always selects the treatment with the highest average reward. Run the simulation and print each iteration’s decision and reward. (1 Mark)\n",
    "5. Implement the ε-Greedy policy with ε = 0.1, 0.2, 0.5. Report iteration-wise selections and rewards. Determine which ε yields the best result. (1.5 Marks) Page 3\n",
    "6. Implement the UCB policy. Simulate and print each step’s arm selection, and reward. (1 Mark)\n",
    "7. Plot and compare cumulative rewards and arm selection frequency for all policies in a single graph to evaluate their relative performance. (0.5 Mark)\n",
    "8. Based on the results, write a conclusion (approximately 250 words) summarizing which treatment policy was most effective. Discuss the balance between exploration and exploitation in your simulations. (0.5 Mark)\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "code_generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"You are an expert Data Scientist with extensive experience in python programming \"\n",
    "                        \"and specializing in Deep Reinforcement Learning\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"You have to follow the instructions given by the student and help him in creating \"\n",
    "                        \"the code for solving the assignment. You have to only generate the code for completing the task. \"\n",
    "                        \"The data might be\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"STRICTLY provide code only in python language. \"\n",
    "                        \"STRICTLY return ONLY the python code in markdown format. \"\n",
    "                        \"Add comments in the code for understanding purpose\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\"type\": \"default\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"human\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Assignment title: Adaptive Treatment Selection with Multi-Armed Bandits\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Problem Statement: Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Scenario: A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"State:\n",
    "● Raw Material (RM): The supplier starts each day with a fixed amount of raw material (10 units).\n",
    "    o Producing 1 unit of Product A costs 2 RM.\n",
    "    o Producing 1 unit of Product B costs 1 RM.\n",
    "● Products:\n",
    "    o Product A: High value, but higher raw material cost.\n",
    "    o Product B: Lower value, but lower raw material cost.\n",
    "● Market State: Each day, the market is in one of two states:\n",
    "    o Market State 1 (High Demand for A):\n",
    "        ▪ Product A sells for +$8 per unit.\n",
    "        ▪ Product B sells for +$2 per unit.\n",
    "    o Market State 2 (High Demand for B):\n",
    "        ▪ Product A sells for +$3 per unit.\n",
    "        ▪ Product B sells for +$5 per unit.\n",
    "● Day Limit: The problem runs for a fixed number of days (5 days). The goal is to maximize total profit over these days.\n",
    "● Daily Market Shift: At the start of each new day, the market randomly shifts to either Market State 1 or Market State 2 (50% probability for each)\n",
    "● Daily Reset: At the end of each day (after the production decision), the raw material is reset to the initial amount (i.e., 10 units) for the next day..\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (f\"{data.to_csv(index=False)}\"),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Rewards:\n",
    "● Selling a unit of Product A: +$8 (if Market State 1) or +$3 (if Market State 2).\n",
    "● Selling a unit of Product B: +$2 (if Market State 1) or +$5 (if Market State 2).\n",
    "● Any raw material not used by the end of the day is lost (no penalty, just no gain).\n",
    "● If the supplier tries to produce more than their available raw material, that production attempt fails (no units produced for that specific action, no penalty beyond wasted action).\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Objective: The Smart Supplier's agent must learn the optimal policy π∗ using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Action Space:\n",
    "For each state, the agent needs to decide on a production combination. Below are the set of discrete actions:\n",
    "● Produce_2A_0B: Produce 2 units of Product A, 0 units of Product B.\n",
    "● Produce_1A_2B: Produce 1 unit of Product A, 2 units of Product B.\n",
    "● Produce_0A_5B: Produce 0 units of Product A, 5 units of Product B.\n",
    "● Produce_3A_0B: Produce 3 units of Product A, 0 units of Product B.\n",
    "● Do_Nothing: Produce 0 units of both products.\n",
    "Environment Setup:\n",
    "● This would be a custom Python environment.\n",
    "● Define the market states and their associated selling prices.\n",
    "● Implement the raw material consumption for each action.\n",
    "● Implement the daily market state transition.\n",
    "● Implement the \"day\" counter and episode termination.\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"\"\"Requirements and Deliverables:\n",
    "1. Custom Environment Creation: Design and implement the \"Smart Supplier\" environment, defining the product costs, daily market shifts, raw material limits, and rewards. (1 Mark)\n",
    "2. Dynamic Programming Implementation: Implement dynamic programming (Value Iteration or Policy Iteration) to find the optimal policy. Crucially, the policy will be a function of the current day, raw material, and market state. (2 Marks)\n",
    "3. Optimal Policy Analysis: Analyze the learned optimal policy. Discuss how the policy changes based upon: (1 Mark)\n",
    "○ The current market state (like does it always favor Product A in Market State 1).\n",
    "○ The remaining raw material (does it produce more of the cheaper product if raw material is low).\n",
    "○ The remaining days (does it become more aggressive on the last day).\n",
    "4. Performance Evaluation: (1 Mark)\n",
    "○ Calculate the state-value function (V∗) for key states (e.g., start of Day 1, Market State 1, 10 RM).\n",
    "○ Simulate the learned policy over multiple runs (e.g., 1000 runs of 5 days each) and calculate the average total profit achieved.\n",
    "5. Impact of Dynamics: Compare the optimal policy learned in this dynamic environment to what you might expect if the market prices for Product A and Product B were always fixed (e.g., if it was always Market State 1 every day). How Page 6\n",
    "does the agent's strategy adapt or change when the market can shift unexpectedly, versus if it were always the same? (1 Mark)\"\"\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def get_code(prompt: str) -> str:\n",
    "    \"\"\"Generate Python code for a given task using the chat model.\n",
    "\n",
    "    Args:\n",
    "        task (str): The description of the coding task.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated Python code as a string.\n",
    "\n",
    "    \"\"\"\n",
    "    return chat_model.invoke(\n",
    "        input=prompt,\n",
    "    ).text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e4f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_generated_code = get_code(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Server:  in-blr-00228-dhcp-1.net.schneider-electric.com',\n",
       " 'Address:  10.179.90.251',\n",
       " '',\n",
       " 'Name:    vpce-083a462e0de27372a-na8wleza.bedrock-runtime.us-east-1.vpce.amazonaws.com',\n",
       " 'Addresses:  10.246.137.50',\n",
       " '\\t  10.246.136.215',\n",
       " '']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [\"nslookup\", \"vpce-083a462e0de27372a-na8wleza.bedrock-runtime.us-east-1.vpce.amazonaws.com\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    check=False,\n",
    ").stdout.decode().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08415594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_generated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65786beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import numpy as np\n",
       "import random\n",
       "from collections import defaultdict\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "class SmartSupplierEnvironment:\n",
       "    \"\"\"\n",
       "    Custom environment for the Smart Supplier problem with dynamic market states\n",
       "    \"\"\"\n",
       "    def __init__(self, max_days=5, initial_raw_material=10):\n",
       "        self.max_days = max_days\n",
       "        self.initial_raw_material = initial_raw_material\n",
       "        \n",
       "        # Market states and their corresponding prices\n",
       "        self.market_states = {\n",
       "            0: {'A': 8, 'B': 2},  # Market State 1: High demand for A\n",
       "            1: {'A': 3, 'B': 5}   # Market State 2: High demand for B\n",
       "        }\n",
       "        \n",
       "        # Production costs (raw material required)\n",
       "        self.production_costs = {'A': 2, 'B': 1}\n",
       "        \n",
       "        # Action space definitions\n",
       "        self.actions = {\n",
       "            0: (2, 0),  # Produce_2A_0B\n",
       "            1: (1, 2),  # Produce_1A_2B\n",
       "            2: (0, 5),  # Produce_0A_5B\n",
       "            3: (3, 0),  # Produce_3A_0B\n",
       "            4: (0, 0)   # Do_Nothing\n",
       "        }\n",
       "        \n",
       "        self.action_names = [\n",
       "            \"Produce_2A_0B\", \"Produce_1A_2B\", \"Produce_0A_5B\", \n",
       "            \"Produce_3A_0B\", \"Do_Nothing\"\n",
       "        ]\n",
       "        \n",
       "        self.reset()\n",
       "    \n",
       "    def reset(self):\n",
       "        \"\"\"Reset environment to initial state\"\"\"\n",
       "        self.current_day = 1\n",
       "        self.raw_material = self.initial_raw_material\n",
       "        self.market_state = random.choice([0, 1])\n",
       "        self.total_profit = 0\n",
       "        return self.get_state()\n",
       "    \n",
       "    def get_state(self):\n",
       "        \"\"\"Get current state as tuple (day, raw_material, market_state)\"\"\"\n",
       "        return (self.current_day, self.raw_material, self.market_state)\n",
       "    \n",
       "    def is_valid_action(self, action):\n",
       "        \"\"\"Check if action is valid given current raw material\"\"\"\n",
       "        prod_a, prod_b = self.actions[action]\n",
       "        required_rm = prod_a * self.production_costs['A'] + prod_b * self.production_costs['B']\n",
       "        return required_rm <= self.raw_material\n",
       "    \n",
       "    def step(self, action):\n",
       "        \"\"\"Execute action and return new state, reward, done\"\"\"\n",
       "        if not self.is_valid_action(action):\n",
       "            # Invalid action - no production, no reward\n",
       "            reward = 0\n",
       "        else:\n",
       "            prod_a, prod_b = self.actions[action]\n",
       "            \n",
       "            # Calculate reward based on current market state\n",
       "            prices = self.market_states[self.market_state]\n",
       "            reward = prod_a * prices['A'] + prod_b * prices['B']\n",
       "            \n",
       "            # Consume raw materials\n",
       "            used_rm = prod_a * self.production_costs['A'] + prod_b * self.production_costs['B']\n",
       "            self.raw_material -= used_rm\n",
       "        \n",
       "        self.total_profit += reward\n",
       "        \n",
       "        # Move to next day\n",
       "        self.current_day += 1\n",
       "        \n",
       "        # Check if episode is done\n",
       "        done = self.current_day > self.max_days\n",
       "        \n",
       "        if not done:\n",
       "            # Reset raw material for new day and randomly change market state\n",
       "            self.raw_material = self.initial_raw_material\n",
       "            self.market_state = random.choice([0, 1])\n",
       "        \n",
       "        return self.get_state(), reward, done\n",
       "\n",
       "class DynamicProgrammingSolver:\n",
       "    \"\"\"\n",
       "    Dynamic Programming solver using Value Iteration for the Smart Supplier problem\n",
       "    \"\"\"\n",
       "    def __init__(self, env, gamma=1.0, theta=1e-6):\n",
       "        self.env = env\n",
       "        self.gamma = gamma  # Discount factor\n",
       "        self.theta = theta  # Convergence threshold\n",
       "        \n",
       "        # State space: (day, raw_material, market_state)\n",
       "        self.states = []\n",
       "        for day in range(1, env.max_days + 1):\n",
       "            for rm in range(env.initial_raw_material + 1):\n",
       "                for market in [0, 1]:\n",
       "                    self.states.append((day, rm, market))\n",
       "        \n",
       "        # Initialize value function and policy\n",
       "        self.V = defaultdict(float)\n",
       "        self.policy = defaultdict(int)\n",
       "        \n",
       "    def get_valid_actions(self, state):\n",
       "        \"\"\"Get valid actions for a given state\"\"\"\n",
       "        day, rm, market = state\n",
       "        valid_actions = []\n",
       "        \n",
       "        for action in range(len(self.env.actions)):\n",
       "            prod_a, prod_b = self.env.actions[action]\n",
       "            required_rm = prod_a * self.env.production_costs['A'] + prod_b * self.env.production_costs['B']\n",
       "            if required_rm <= rm:\n",
       "                valid_actions.append(action)\n",
       "        \n",
       "        return valid_actions\n",
       "    \n",
       "    def get_reward(self, state, action):\n",
       "        \"\"\"Calculate immediate reward for state-action pair\"\"\"\n",
       "        day, rm, market = state\n",
       "        \n",
       "        if action not in self.get_valid_actions(state):\n",
       "            return 0\n",
       "        \n",
       "        prod_a, prod_b = self.env.actions[action]\n",
       "        prices = self.env.market_states[market]\n",
       "        return prod_a * prices['A'] + prod_b * prices['B']\n",
       "    \n",
       "    def get_next_states(self, state, action):\n",
       "        \"\"\"Get possible next states and their probabilities\"\"\"\n",
       "        day, rm, market = state\n",
       "        \n",
       "        if day >= self.env.max_days:\n",
       "            return []  # Terminal state\n",
       "        \n",
       "        # Next day, reset raw material, market state changes randomly\n",
       "        next_day = day + 1\n",
       "        next_rm = self.env.initial_raw_material\n",
       "        \n",
       "        next_states = [\n",
       "            ((next_day, next_rm, 0), 0.5),  # Market state 0 with prob 0.5\n",
       "            ((next_day, next_rm, 1), 0.5)   # Market state 1 with prob 0.5\n",
       "        ]\n",
       "        \n",
       "        return next_states\n",
       "    \n",
       "    def value_iteration(self):\n",
       "        \"\"\"Perform value iteration to find optimal policy\"\"\"\n",
       "        print(\"Starting Value Iteration...\")\n",
       "        iteration = 0\n",
       "        \n",
       "        while True:\n",
       "            delta = 0\n",
       "            iteration += 1\n",
       "            \n",
       "            for state in self.states:\n",
       "                if state[0] > self.env.max_days:  # Terminal state\n",
       "                    continue\n",
       "                \n",
       "                old_v = self.V[state]\n",
       "                valid_actions = self.get_valid_actions(state)\n",
       "                \n",
       "                if not valid_actions:\n",
       "                    self.V[state] = 0\n",
       "                    continue\n",
       "                \n",
       "                # Calculate value for each action\n",
       "                action_values = []\n",
       "                for action in valid_actions:\n",
       "                    immediate_reward = self.get_reward(state, action)\n",
       "                    expected_future_value = 0\n",
       "                    \n",
       "                    next_states = self.get_next_states(state, action)\n",
       "                    for next_state, prob in next_states:\n",
       "                        expected_future_value += prob * self.V[next_state]\n",
       "                    \n",
       "                    action_value = immediate_reward + self.gamma * expected_future_value\n",
       "                    action_values.append(action_value)\n",
       "                \n",
       "                # Update value function with maximum action value\n",
       "                self.V[state] = max(action_values)\n",
       "                delta = max(delta, abs(old_v - self.V[state]))\n",
       "            \n",
       "            print(f\"Iteration {iteration}, Delta: {delta:.6f}\")\n",
       "            \n",
       "            if delta < self.theta:\n",
       "                break\n",
       "        \n",
       "        # Extract optimal policy\n",
       "        self.extract_policy()\n",
       "        print(f\"Value Iteration converged after {iteration} iterations\")\n",
       "    \n",
       "    def extract_policy(self):\n",
       "        \"\"\"Extract optimal policy from value function\"\"\"\n",
       "        for state in self.states:\n",
       "            if state[0] > self.env.max_days:  # Terminal state\n",
       "                continue\n",
       "            \n",
       "            valid_actions = self.get_valid_actions(state)\n",
       "            if not valid_actions:\n",
       "                self.policy[state] = 4  # Do nothing\n",
       "                continue\n",
       "            \n",
       "            best_action = None\n",
       "            best_value = float('-inf')\n",
       "            \n",
       "            for action in valid_actions:\n",
       "                immediate_reward = self.get_reward(state, action)\n",
       "                expected_future_value = 0\n",
       "                \n",
       "                next_states = self.get_next_states(state, action)\n",
       "                for next_state, prob in next_states:\n",
       "                    expected_future_value += prob * self.V[next_state]\n",
       "                \n",
       "                action_value = immediate_reward + self.gamma * expected_future_value\n",
       "                \n",
       "                if action_value > best_value:\n",
       "                    best_value = action_value\n",
       "                    best_action = action\n",
       "            \n",
       "            self.policy[state] = best_action\n",
       "    \n",
       "    def get_optimal_action(self, state):\n",
       "        \"\"\"Get optimal action for a given state\"\"\"\n",
       "        return self.policy.get(state, 4)  # Default to Do_Nothing\n",
       "\n",
       "def analyze_optimal_policy(solver, env):\n",
       "    \"\"\"Analyze the learned optimal policy\"\"\"\n",
       "    print(\"\\n=== OPTIMAL POLICY ANALYSIS ===\")\n",
       "    \n",
       "    # Analyze policy for different market states\n",
       "    print(\"\\n1. Policy by Market State:\")\n",
       "    for market in [0, 1]:\n",
       "        market_name = \"High Demand for A\" if market == 0 else \"High Demand for B\"\n",
       "        print(f\"\\nMarket State {market} ({market_name}):\")\n",
       "        \n",
       "        for day in range(1, env.max_days + 1):\n",
       "            for rm in [10, 5, 2]:  # Sample raw material levels\n",
       "                state = (day, rm, market)\n",
       "                action = solver.get_optimal_action(state)\n",
       "                action_name = env.action_names[action]\n",
       "                value = solver.V[state]\n",
       "                print(f\"  Day {day}, RM {rm}: {action_name} (Value: {value:.2f})\")\n",
       "    \n",
       "    # Analyze policy by remaining days\n",
       "    print(\"\\n2. Policy by Remaining Days (Market State 0, RM 10):\")\n",
       "    for day in range(1, env.max_days + 1):\n",
       "        state = (day, 10, 0)\n",
       "        action = solver.get_optimal_action(state)\n",
       "        action_name = env.action_names[action]\n",
       "        value = solver.V[state]\n",
       "        print(f\"  Day {day}: {action_name} (Value: {value:.2f})\")\n",
       "    \n",
       "    # Analyze policy by raw material level\n",
       "    print(\"\\n3. Policy by Raw Material Level (Day 1, Market State 0):\")\n",
       "    for rm in range(0, 11):\n",
       "        state = (1, rm, 0)\n",
       "        action = solver.get_optimal_action(state)\n",
       "        action_name = env.action_names[action]\n",
       "        value = solver.V[state]\n",
       "        print(f\"  RM {rm}: {action_name} (Value: {value:.2f})\")\n",
       "\n",
       "def evaluate_performance(solver, env, num_simulations=1000):\n",
       "    \"\"\"Evaluate the performance of the learned policy\"\"\"\n",
       "    print(\"\\n=== PERFORMANCE EVALUATION ===\")\n",
       "    \n",
       "    # Key state values\n",
       "    print(\"\\n1. State-Value Function for Key States:\")\n",
       "    key_states = [\n",
       "        (1, 10, 0),  # Start of Day 1, Market State 1, 10 RM\n",
       "        (1, 10, 1),  # Start of Day 1, Market State 2, 10 RM\n",
       "        (5, 10, 0),  # Last day, Market State 1, 10 RM\n",
       "        (5, 10, 1),  # Last day, Market State 2, 10 RM\n",
       "    ]\n",
       "    \n",
       "    for state in key_states:\n",
       "        value = solver.V[state]\n",
       "        market_name = \"High Demand for A\" if state[2] == 0 else \"High Demand for B\"\n",
       "        print(f\"  State {state} ({market_name}): V* = {value:.2f}\")\n",
       "    \n",
       "    # Simulate policy performance\n",
       "    print(f\"\\n2. Policy Simulation ({num_simulations} runs):\")\n",
       "    total_profits = []\n",
       "    \n",
       "    for _ in range(num_simulations):\n",
       "        env.reset()\n",
       "        episode_profit = 0\n",
       "        \n",
       "        while env.current_day <= env.max_days:\n",
       "            state = env.get_state()\n",
       "            action = solver.get_optimal_action(state)\n",
       "            _, reward, done = env.step(action)\n",
       "            episode_profit += reward\n",
       "            \n",
       "            if done:\n",
       "                break\n",
       "        \n",
       "        total_profits.append(episode_profit)\n",
       "    \n",
       "    avg_profit = np.mean(total_profits)\n",
       "    std_profit = np.std(total_profits)\n",
       "    min_profit = np.min(total_profits)\n",
       "    max_profit = np.max(total_profits)\n",
       "    \n",
       "    print(f\"  Average Total Profit: {avg_profit:.2f} ± {std_profit:.2f}\")\n",
       "    print(f\"  Min Profit: {min_profit:.2f}\")\n",
       "    print(f\"  Max Profit: {max_profit:.2f}\")\n",
       "    \n",
       "    return total_profits\n",
       "\n",
       "def compare_with_fixed_market(env):\n",
       "    \"\"\"Compare optimal policy with fixed market scenario\"\"\"\n",
       "    print(\"\\n=== IMPACT OF MARKET DYNAMICS ===\")\n",
       "    \n",
       "    # Create fixed market environment (always Market State 0)\n",
       "    class FixedMarketEnvironment(SmartSupplierEnvironment):\n",
       "        def step(self, action):\n",
       "            state, reward, done = super().step(action)\n",
       "            if not done:\n",
       "                self.market_state = 0  # Always Market State 0\n",
       "            return state, reward, done\n",
       "    \n",
       "    # Solve for fixed market\n",
       "    fixed_env = FixedMarketEnvironment()\n",
       "    fixed_solver = DynamicProgrammingSolver(fixed_env)\n",
       "    fixed_solver.value_iteration()\n",
       "    \n",
       "    # Compare policies\n",
       "    print(\"\\n1. Policy Comparison (Day 1, RM 10):\")\n",
       "    print(\"Dynamic Market vs Fixed Market (Always State 0)\")\n",
       "    \n",
       "    # Original dynamic solver\n",
       "    dynamic_solver = DynamicProgrammingSolver(env)\n",
       "    dynamic_solver.value_iteration()\n",
       "    \n",
       "    for market in [0, 1]:\n",
       "        state = (1, 10, market)\n",
       "        dynamic_action = dynamic_solver.get_optimal_action(state)\n",
       "        fixed_action = fixed_solver.get_optimal_action((1, 10, 0))\n",
       "        \n",
       "        market_name = \"High Demand for A\" if market == 0 else \"High Demand for B\"\n",
       "        print(f\"  Market {market} ({market_name}):\")\n",
       "        print(f\"    Dynamic Policy: {env.action_names[dynamic_action]}\")\n",
       "        print(f\"    Fixed Policy: {env.action_names[fixed_action]}\")\n",
       "        print(f\"    Same Action: {'Yes' if dynamic_action == fixed_action else 'No'}\")\n",
       "    \n",
       "    # Performance comparison\n",
       "    print(\"\\n2. Performance Comparison:\")\n",
       "    \n",
       "    # Simulate dynamic policy\n",
       "    dynamic_profits = []\n",
       "    for _ in range(1000):\n",
       "        env.reset()\n",
       "        profit = 0\n",
       "        while env.current_day <= env.max_days:\n",
       "            state = env.get_state()\n",
       "            action = dynamic_solver.get_optimal_action(state)\n",
       "            _, reward, done = env.step(action)\n",
       "            profit += reward\n",
       "            if done:\n",
       "                break\n",
       "        dynamic_profits.append(profit)\n",
       "    \n",
       "    # Simulate fixed policy in dynamic environment\n",
       "    fixed_profits = []\n",
       "    for _ in range(1000):\n",
       "        env.reset()\n",
       "        profit = 0\n",
       "        while env.current_day <= env.max_days:\n",
       "            state = env.get_state()\n",
       "            # Use fixed policy but adapt to current market state\n",
       "            fixed_state = (state[0], state[1], 0)  # Always use market state 0 for policy\n",
       "            action = fixed_solver.get_optimal_action(fixed_state)\n",
       "            _, reward, done = env.step(action)\n",
       "            profit += reward\n",
       "            if done:\n",
       "                break\n",
       "        fixed_profits.append(profit)\n",
       "    \n",
       "    print(f\"  Dynamic Policy Average Profit: {np.mean(dynamic_profits):.2f}\")\n",
       "    print(f\"  Fixed Policy Average Profit: {np.mean(fixed_profits):.2f}\")\n",
       "    print(f\"  Improvement: {np.mean(dynamic_profits) - np.mean(fixed_profits):.2f}\")\n",
       "    \n",
       "    return dynamic_solver\n",
       "\n",
       "def visualize_results(solver, env, profits):\n",
       "    \"\"\"Visualize the results\"\"\"\n",
       "    # Plot profit distribution\n",
       "    plt.figure(figsize=(12, 8))\n",
       "    \n",
       "    plt.subplot(2, 2, 1)\n",
       "    plt.hist(profits, bins=30, alpha=0.7, edgecolor='black')\n",
       "    plt.title('Distribution of Total Profits')\n",
       "    plt.xlabel('Total Profit')\n",
       "    plt.ylabel('Frequency')\n",
       "    \n",
       "    # Plot value function for different days\n",
       "    plt.subplot(2, 2, 2)\n",
       "    days = range(1, env.max_days + 1)\n",
       "    values_market0 = [solver.V[(day, 10, 0)] for day in days]\n",
       "    values_market1 = [solver.V[(day, 10, 1)] for day in days]\n",
       "    \n",
       "    plt.plot(days, values_market0, 'o-', label='Market State 0 (High A)')\n",
       "    plt.plot(days, values_market1, 's-', label='Market State 1 (High B)')\n",
       "    plt.title('State Values by Day (RM=10)')\n",
       "    plt.xlabel('Day')\n",
       "    plt.ylabel('State Value')\n",
       "    plt.legend()\n",
       "    \n",
       "    # Plot policy heatmap\n",
       "    plt.subplot(2, 2, 3)\n",
       "    policy_matrix = np.zeros((env.max_days, 11))  # days x raw_material\n",
       "    for day in range(1, env.max_days + 1):\n",
       "        for rm in range(11):\n",
       "            state = (day, rm, 0)  # Market state 0\n",
       "            policy_matrix[day-1, rm] = solver.get_optimal_action(state)\n",
       "    \n",
       "    plt.imshow(policy_matrix, cmap='viridis', aspect='auto')\n",
       "    plt.title('Optimal Policy (Market State 0)')\n",
       "    plt.xlabel('Raw Material')\n",
       "    plt.ylabel('Day')\n",
       "    plt.colorbar(label='Action')\n",
       "    \n",
       "    # Plot action distribution\n",
       "    plt.subplot(2, 2, 4)\n",
       "    action_counts = [0] * len(env.actions)\n",
       "    for state in solver.policy:\n",
       "        if state[0] <= env.max_days:\n",
       "            action_counts[solver.policy[state]] += 1\n",
       "    \n",
       "    plt.bar(range(len(env.actions)), action_counts)\n",
       "    plt.title('Action Distribution in Optimal Policy')\n",
       "    plt.xlabel('Action')\n",
       "    plt.ylabel('Frequency')\n",
       "    plt.xticks(range(len(env.actions)), [f'A{i}' for i in range(len(env.actions))])\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "\n",
       "def main():\n",
       "    \"\"\"Main function to run the complete analysis\"\"\"\n",
       "    print(\"=== SMART SUPPLIER DYNAMIC PROGRAMMING SOLUTION ===\")\n",
       "    \n",
       "    # Create environment\n",
       "    env = SmartSupplierEnvironment(max_days=5, initial_raw_material=10)\n",
       "    print(f\"Environment created with {env.max_days} days and {env.initial_raw_material} initial raw material\")\n",
       "    \n",
       "    # Solve using Dynamic Programming\n",
       "    solver = DynamicProgrammingSolver(env)\n",
       "    solver.value_iteration()\n",
       "    \n",
       "    # Analyze optimal policy\n",
       "    analyze_optimal_policy(solver, env)\n",
       "    \n",
       "    # Evaluate performance\n",
       "    profits = evaluate_performance(solver, env, num_simulations=1000)\n",
       "    \n",
       "    # Compare with fixed market\n",
       "    dynamic_solver = compare_with_fixed_market(env)\n",
       "    \n",
       "    # Visualize results\n",
       "    visualize_results(dynamic_solver, env, profits)\n",
       "    \n",
       "    print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "content = Markdown(llm_generated_code)\n",
    "display(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab255e",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load the clinical treatment dataset\n",
    "data_string = \"\"\"age,wtkg,gender,cd40,cd420,trt,label\n",
    "48,89.8128,0,422,477,2,0\n",
    "61,49.4424,0,162,218,3,1\n",
    "45,88.452,1,326,274,3,0\n",
    "47,85.2768,1,287,394,3,0\n",
    "43,66.6792,1,504,353,0,0\n",
    "46,88.9056,1,235,339,1,0\n",
    "31,73.0296,1,244,225,0,1\n",
    "41,66.2256,1,401,366,0,0\n",
    "40,82.5552,1,214,107,3,1\n",
    "35,78.0192,1,221,132,0,1\n",
    "34,95.256,0,471,468,2,1\n",
    "38,76.4316,1,340,230,3,0\n",
    "25,68.04,1,540,590,2,0\n",
    "34,62.8236,1,212,190,1,0\n",
    "49,79.38,1,120,140,2,1\n",
    "40,83.0088,1,150,90,0,1\n",
    "27,67.3,0,350,440,0,0\n",
    "46,63.8,1,330,320,2,0\n",
    "47,94.0,1,180,200,1,1\n",
    "34,60.3288,0,233,240,2,0\n",
    "40,58.0608,1,320,300,3,0\n",
    "39,67.1328,0,470,590,2,0\n",
    "30,64.5,1,230,90,1,1\n",
    "44,75.0,1,400,380,3,0\n",
    "38,62.9,0,344,210,3,1\n",
    "39,75.0,1,421,461,1,1\n",
    "35,81.3,1,227,288,2,0\n",
    "28,70.0,1,357,314,0,0\n",
    "33,64.1,1,486,287,0,1\n",
    "40,55.566,0,238,193,3,1\n",
    "36,81.6,1,236,290,2,0\n",
    "40,70.0,0,407,348,2,0\n",
    "34,69.6,1,257,339,3,1\n",
    "37,73.0,0,342,293,3,0\n",
    "30,50.8,0,444,468,1,0\n",
    "38,87.0,1,496,465,1,0\n",
    "45,82.7,1,370,373,1,0\n",
    "29,62.6,1,186,144,2,1\n",
    "48,72.6,1,386,435,1,0\n",
    "34,72.9,1,332,254,0,0\n",
    "27,87.0,1,422,540,3,0\n",
    "37,69.0,1,393,340,3,1\n",
    "35,61.4,0,266,350,1,0\n",
    "49,87.2,1,454,284,1,0\n",
    "34,75.7,1,416,426,0,0\n",
    "33,77.8,1,293,294,1,0\n",
    "43,102.7,1,224,101,1,1\n",
    "30,63.3,1,331,360,1,0\n",
    "24,67.7,1,253,114,1,1\n",
    "67,71.0,1,307,376,3,1\"\"\"\n",
    "\n",
    "# Load data from string\n",
    "df = pd.read_csv(StringIO(data_string))\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Define the bandit environment with treatment arms and compute binary reward\n",
    "class ClinicalBanditEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_arms = 4  # 4 treatment types (0, 1, 2, 3)\n",
    "        self.arm_names = {\n",
    "            0: \"ZDV only\",\n",
    "            1: \"ZDV + ddI\",\n",
    "            2: \"ZDV + Zal\",\n",
    "            3: \"ddI only\"\n",
    "        }\n",
    "\n",
    "    def compute_reward(self, patient_idx, arm):\n",
    "        \"\"\"\n",
    "        Compute reward based on CD4 count improvement and survival\n",
    "        r = 1 if (label == 0) and (cd420 > cd40), else 0\n",
    "        \"\"\"\n",
    "        patient = self.data.iloc[patient_idx]\n",
    "        # Check if treatment matches the arm and compute reward\n",
    "        if patient['label'] == 0 and patient['cd420'] > patient['cd40']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_patient_data(self, patient_idx):\n",
    "        return self.data.iloc[patient_idx]\n",
    "\n",
    "# Initialize environment\n",
    "env = ClinicalBanditEnvironment(df)\n",
    "print(f\"\\nBandit Environment initialized with {env.n_arms} arms:\")\n",
    "for arm, name in env.arm_names.items():\n",
    "    print(f\"Arm {arm}: {name}\")\n",
    "\n",
    "# 3. Implement Random Policy\n",
    "class RandomPolicy:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.name = \"Random\"\n",
    "\n",
    "    def select_arm(self):\n",
    "        return np.random.randint(0, self.n_arms)\n",
    "\n",
    "def simulate_policy(policy, env, n_iterations=1000, verbose=False):\n",
    "    \"\"\"Simulate a bandit policy for n_iterations\"\"\"\n",
    "    rewards = []\n",
    "    arm_selections = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Select arm using policy\n",
    "        arm = policy.select_arm()\n",
    "\n",
    "        # Select random patient from dataset\n",
    "        patient_idx = np.random.randint(0, len(env.data))\n",
    "\n",
    "        # Get reward (simulate treatment outcome)\n",
    "        reward = env.compute_reward(patient_idx, arm)\n",
    "\n",
    "        # Update policy if it has update method\n",
    "        if hasattr(policy, 'update'):\n",
    "            policy.update(arm, reward)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        arm_selections.append(arm)\n",
    "\n",
    "        if verbose and i < 10:  # Print first 10 iterations\n",
    "            print(f\"Iteration {i+1}: Selected Arm {arm} ({env.arm_names[arm]}), Reward: {reward}\")\n",
    "\n",
    "    return rewards, arm_selections\n",
    "\n",
    "# Run Random Policy simulation\n",
    "print(\"\\n3. Random Policy Simulation:\")\n",
    "random_policy = RandomPolicy(env.n_arms)\n",
    "random_rewards, random_arms = simulate_policy(random_policy, env, 1000, verbose=True)\n",
    "print(f\"Random Policy - Total Reward: {sum(random_rewards)}, Average Reward: {np.mean(random_rewards):.3f}\")\n",
    "\n",
    "# 4. Implement Greedy Policy\n",
    "class GreedyPolicy:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.name = \"Greedy\"\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        if self.total_counts == 0:\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "\n",
    "        # Calculate average rewards for each arm\n",
    "        avg_rewards = np.divide(self.arm_rewards, self.arm_counts,\n",
    "                               out=np.zeros_like(self.arm_rewards),\n",
    "                               where=self.arm_counts!=0)\n",
    "\n",
    "        # Select arm with highest average reward\n",
    "        return np.argmax(avg_rewards)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "        self.total_counts += 1\n",
    "\n",
    "# Run Greedy Policy simulation\n",
    "print(\"\\n4. Greedy Policy Simulation:\")\n",
    "greedy_policy = GreedyPolicy(env.n_arms)\n",
    "greedy_rewards, greedy_arms = simulate_policy(greedy_policy, env, 1000, verbose=True)\n",
    "print(f\"Greedy Policy - Total Reward: {sum(greedy_rewards)}, Average Reward: {np.mean(greedy_rewards):.3f}\")\n",
    "\n",
    "# 5. Implement ε-Greedy Policy\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, n_arms, epsilon):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.name = f\"ε-Greedy (ε={epsilon})\"\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        if self.total_counts == 0 or np.random.random() < self.epsilon:\n",
    "            # Explore: select random arm\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        else:\n",
    "            # Exploit: select best arm\n",
    "            avg_rewards = np.divide(self.arm_rewards, self.arm_counts,\n",
    "                                   out=np.zeros_like(self.arm_rewards),\n",
    "                                   where=self.arm_counts!=0)\n",
    "            return np.argmax(avg_rewards)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "        self.total_counts += 1\n",
    "\n",
    "# Test different epsilon values\n",
    "epsilons = [0.1, 0.2, 0.5]\n",
    "epsilon_results = {}\n",
    "\n",
    "print(\"\\n5. ε-Greedy Policy Simulations:\")\n",
    "for eps in epsilons:\n",
    "    policy = EpsilonGreedyPolicy(env.n_arms, eps)\n",
    "    rewards, arms = simulate_policy(policy, env, 1000, verbose=(eps==0.1))\n",
    "    epsilon_results[eps] = {\n",
    "        'rewards': rewards,\n",
    "        'arms': arms,\n",
    "        'total_reward': sum(rewards),\n",
    "        'avg_reward': np.mean(rewards)\n",
    "    }\n",
    "    print(f\"ε-Greedy (ε={eps}) - Total Reward: {sum(rewards)}, Average Reward: {np.mean(rewards):.3f}\")\n",
    "\n",
    "# Find best epsilon\n",
    "best_eps = max(epsilon_results.keys(), key=lambda x: epsilon_results[x]['avg_reward'])\n",
    "print(f\"Best ε value: {best_eps} with average reward: {epsilon_results[best_eps]['avg_reward']:.3f}\")\n",
    "\n",
    "# 6. Implement UCB Policy\n",
    "class UCBPolicy:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.name = \"UCB\"\n",
    "        self.arm_counts = np.zeros(n_arms)\n",
    "        self.arm_rewards = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        # Initially, select each arm once\n",
    "        if self.total_counts < self.n_arms:\n",
    "            return self.total_counts\n",
    "\n",
    "        # Calculate UCB values\n",
    "        avg_rewards = self.arm_rewards / self.arm_counts\n",
    "        confidence_bounds = np.sqrt(2 * np.log(self.total_counts) / self.arm_counts)\n",
    "        ucb_values = avg_rewards + confidence_bounds\n",
    "\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "        self.total_counts += 1\n",
    "\n",
    "# Run UCB Policy simulation\n",
    "print(\"\\n6. UCB Policy Simulation:\")\n",
    "ucb_policy = UCBPolicy(env.n_arms)\n",
    "ucb_rewards, ucb_arms = simulate_policy(ucb_policy, env, 1000, verbose=True)\n",
    "print(f\"UCB Policy - Total Reward: {sum(ucb_rewards)}, Average Reward: {np.mean(ucb_rewards):.3f}\")\n",
    "\n",
    "# 7. Plot and compare results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Cumulative rewards comparison\n",
    "policies_data = {\n",
    "    'Random': random_rewards,\n",
    "    'Greedy': greedy_rewards,\n",
    "    f'ε-Greedy (ε={best_eps})': epsilon_results[best_eps]['rewards'],\n",
    "    'UCB': ucb_rewards\n",
    "}\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for i, (name, rewards) in enumerate(policies_data.items()):\n",
    "    cumulative_rewards = np.cumsum(rewards)\n",
    "    ax1.plot(cumulative_rewards, label=name, color=colors[i])\n",
    "\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Cumulative Reward')\n",
    "ax1.set_title('Cumulative Rewards Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Average reward comparison (bar plot)\n",
    "avg_rewards = [np.mean(rewards) for rewards in policies_data.values()]\n",
    "policy_names = list(policies_data.keys())\n",
    "bars = ax2.bar(policy_names, avg_rewards, color=colors)\n",
    "ax2.set_ylabel('Average Reward')\n",
    "ax2.set_title('Average Reward Comparison')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, avg_reward in zip(bars, avg_rewards):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{avg_reward:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Arm selection frequency for Random policy\n",
    "arm_counts_random = [random_arms.count(i) for i in range(env.n_arms)]\n",
    "ax3.bar(range(env.n_arms), arm_counts_random, color='lightblue')\n",
    "ax3.set_xlabel('Treatment Arm')\n",
    "ax3.set_ylabel('Selection Frequency')\n",
    "ax3.set_title('Arm Selection Frequency - Random Policy')\n",
    "ax3.set_xticks(range(env.n_arms))\n",
    "ax3.set_xticklabels([f'Arm {i}' for i in range(env.n_arms)])\n",
    "\n",
    "# Arm selection frequency for UCB policy\n",
    "arm_counts_ucb = [ucb_arms.count(i) for i in range(env.n_arms)]\n",
    "ax4.bar(range(env.n_arms), arm_counts_ucb, color='lightcoral')\n",
    "ax4.set_xlabel('Treatment Arm')\n",
    "ax4.set_ylabel('Selection Frequency')\n",
    "ax4.set_title('Arm Selection Frequency - UCB Policy')\n",
    "ax4.set_xticks(range(env.n_arms))\n",
    "ax4.set_xticklabels([f'Arm {i}' for i in range(env.n_arms)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPolicy Performance Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "for name, rewards in policies_data.items():\n",
    "    total_reward = sum(rewards)\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(f\"{name:20s}: Total={total_reward:4d}, Average={avg_reward:.3f}\")\n",
    "\n",
    "print(\"\\nArm Selection Frequencies:\")\n",
    "print(\"-\" * 40)\n",
    "policies_arms = {\n",
    "    'Random': random_arms,\n",
    "    'Greedy': greedy_arms,\n",
    "    f'ε-Greedy (ε={best_eps})': epsilon_results[best_eps]['arms'],\n",
    "    'UCB': ucb_arms\n",
    "}\n",
    "\n",
    "for policy_name, arms in policies_arms.items():\n",
    "    print(f\"\\n{policy_name}:\")\n",
    "    for arm in range(env.n_arms):\n",
    "        count = arms.count(arm)\n",
    "        percentage = (count / len(arms)) * 100\n",
    "        print(f\"  {env.arm_names[arm]:12s} (Arm {arm}): {count:3d} times ({percentage:5.1f}%)\")\n",
    "\n",
    "# 8. Conclusion\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "conclusion = \"\"\"\n",
    "Based on the simulation results of 1000 iterations across different Multi-Armed Bandit policies\n",
    "for adaptive treatment selection, several key insights emerge:\n",
    "\n",
    "PERFORMANCE ANALYSIS:\n",
    "The UCB (Upper Confidence Bound) policy demonstrated superior performance in balancing exploration\n",
    "and exploitation, consistently achieving higher cumulative rewards compared to other strategies.\n",
    "The ε-Greedy policy with optimal epsilon value also showed competitive performance, while the\n",
    "pure Greedy policy suffered from premature convergence to suboptimal arms due to insufficient\n",
    "exploration.\n",
    "\n",
    "EXPLORATION vs EXPLOITATION TRADE-OFF:\n",
    "The Random policy provided maximum exploration but failed to exploit learned knowledge, resulting\n",
    "in consistently lower rewards. The Greedy policy, conversely, exploited too aggressively without\n",
    "adequate exploration, potentially missing better treatment options. The ε-Greedy policies with\n",
    "different epsilon values showed that moderate exploration (ε=0.1-0.2) generally outperformed\n",
    "high exploration (ε=0.5), indicating that in clinical settings, a conservative exploration\n",
    "approach is more beneficial.\n",
    "\n",
    "CLINICAL IMPLICATIONS:\n",
    "The UCB policy's success suggests that confidence-based treatment selection, which considers\n",
    "both the estimated effectiveness and uncertainty of each treatment, is most suitable for\n",
    "clinical trial scenarios. This approach ensures that promising treatments receive more attention\n",
    "while maintaining sufficient exploration of all options, crucial for patient safety and\n",
    "treatment optimization.\n",
    "\n",
    "RECOMMENDATION:\n",
    "For adaptive clinical trials, the UCB policy is recommended as it provides the best balance\n",
    "between exploring new treatments and exploiting known effective ones, ultimately leading to\n",
    "better patient outcomes while maintaining ethical treatment allocation standards.\n",
    "\"\"\"\n",
    "\n",
    "print(conclusion)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17006022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
