{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5de6b4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import difflib\n",
    "import string\n",
    "from functools import cached_property\n",
    "from pathlib import Path\n",
    "\n",
    "import docx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary package for nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "870fcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"A class representing a processed document with tokens and optional name.\"\"\"\n",
    "\n",
    "    TOTAL_DOCUMENTS = 0\n",
    "\n",
    "    def __init__(self, tokens: list[str], name: str | None = None) -> None:  # noqa: D107\n",
    "        self.id_ = self.generate_id()\n",
    "        self.tokens = tokens\n",
    "        self.name = name\n",
    "\n",
    "    @classmethod\n",
    "    def generate_id(cls) -> int:\n",
    "        \"\"\"Generate a unique ID for each document instance.\n",
    "\n",
    "        Returns:\n",
    "            int: The unique document ID.\n",
    "\n",
    "        \"\"\"\n",
    "        cls.TOTAL_DOCUMENTS += 1\n",
    "        return cls.TOTAL_DOCUMENTS\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"A class for preprocessing documents for information retrieval tasks.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:  # noqa: D107\n",
    "        # Getting english stop words\n",
    "        self._stop_words = {*stopwords.words(fileids=\"english\")}\n",
    "        # Creating a word net lemmatizer\n",
    "        # Using lemmatizer as the corpus size for this assignment is less\n",
    "        self._word_net_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self._word_net_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return nltk.tokenize.word_tokenize(text=text, language=\"english\")\n",
    "\n",
    "    def _lower_case(self, text: str) -> str:\n",
    "        return text.lower()\n",
    "\n",
    "    def _is_punctuation(self, token: str) -> bool:\n",
    "        return token in string.punctuation\n",
    "\n",
    "    def _is_stop_word(self, token: str) -> bool:\n",
    "        return token in self._stop_words - {\"and\", \"not\", \"or\"}\n",
    "\n",
    "    def _lemmatize(self, token: str) -> str:\n",
    "        return self._word_net_lemmatizer.lemmatize(token)\n",
    "\n",
    "    def _read_text(self, document_path: Path) -> str:\n",
    "        with document_path.open(\"r\", encoding=\"utf-8\") as fp:\n",
    "            return fp.read()\n",
    "\n",
    "    def _read_csv(self, document_path: Path) -> str:\n",
    "        # Read CSV files as DataFrame and return as space separated strings without the headers\n",
    "        return pd.read_csv(document_path).to_csv(index=False, header=False, sep=\" \")\n",
    "\n",
    "    def _read_excel(self, document_path: Path) -> str:\n",
    "        # Read excel files and return as space separated strings without the headers\n",
    "        return pd.read_excel(document_path).to_csv(index=False, header=False, sep=\" \")\n",
    "\n",
    "    def _read_pdf(self, document_path: Path) -> str:\n",
    "        with document_path.open(\"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            return \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "    def _read_docx(self, document_path: Path) -> str:\n",
    "        doc = docx.Document(document_path)\n",
    "        return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "\n",
    "    def _read_document(self, document_path: Path) -> str:\n",
    "        _read_function_map = {\n",
    "            \".txt\": self._read_text,\n",
    "            \".xlsx\": self._read_excel,\n",
    "            \".xls\": self._read_excel,\n",
    "            \".csv\": self._read_csv,\n",
    "            \".pdf\": self._read_pdf,\n",
    "            \".docx\": self._read_docx,\n",
    "        }\n",
    "        document_extension = document_path.suffix\n",
    "\n",
    "        if document_extension in _read_function_map:\n",
    "            return _read_function_map[document_extension](document_path=document_path)\n",
    "        msg = f\"{document_extension} document type is not supported.\"\n",
    "        raise TypeError(msg)\n",
    "\n",
    "    def process_document(self, document_path: Path) -> Document:\n",
    "        \"\"\"Preprocess a document for indexing.\n",
    "\n",
    "        Process the document by reading, normalizing, tokenizing, removing stopwords and punctuation,\n",
    "        lemmatizing, and returning a Document object.\n",
    "\n",
    "        Returns:\n",
    "            Document: The processed Document object containing lemmatized tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        text = self._read_document(document_path=document_path)\n",
    "        lemmatized_tokens = self.process_text(text=text)\n",
    "        return Document(tokens=lemmatized_tokens, name=document_path.stem)\n",
    "\n",
    "    def process_text(self, text: str) -> list[str]:\n",
    "        \"\"\"Process raw text by normalizing, tokenizing, removing stopwords and punctuation, and lemmatizing.\n",
    "\n",
    "        Args:\n",
    "            text (str): The raw text to process.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of lemmatized tokens after preprocessing.\n",
    "\n",
    "        \"\"\"\n",
    "        lower_case_text = self._lower_case(text=text)\n",
    "        replace_punctuations = lower_case_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        tokens = self._tokenize(text=replace_punctuations)\n",
    "\n",
    "        # Remove punctuations and stop words\n",
    "        filtered_tokens = filter(\n",
    "            lambda token: not (self._is_stop_word(token=token)),\n",
    "            tokens,\n",
    "        )\n",
    "\n",
    "        # Lemmatized words\n",
    "        return list(map(self._lemmatize, filtered_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "28089751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class PositionalInvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.__index = defaultdict(lambda: defaultdict(list))\n",
    "        self.__term_frequencies = defaultdict(int)\n",
    "        self.__documents = {}\n",
    "\n",
    "    def add_document(self, document: Document):\n",
    "        for position, token in enumerate(document.tokens):\n",
    "            self.__index[token][document.id_].append(position)\n",
    "            self.__term_frequencies[token] += 1\n",
    "        self.__documents[document.id_] = document.name\n",
    "\n",
    "    def get_posting_list(self, term: str):\n",
    "        return self.__index[term]\n",
    "\n",
    "    def get_term_frequency(self, term: str):\n",
    "        return self.__term_frequencies[term]\n",
    "\n",
    "    @property\n",
    "    def documents(self) -> dict[int, str]:\n",
    "        return self.__documents\n",
    "\n",
    "    @property\n",
    "    def terms(self) -> list[str]:\n",
    "        return self.__index.keys()\n",
    "\n",
    "    def __contains__(self, term: str):\n",
    "        return term in self.__index\n",
    "\n",
    "    def __getitem__(self, term: str):\n",
    "        return self.__index[term]\n",
    "\n",
    "\n",
    "class MedicalSearchEngine:\n",
    "    def __init__(self, positional_index: PositionalInvertedIndex, preprocessor: Preprocessor):\n",
    "        self.__positional_index = positional_index\n",
    "        self.__preprocessor = preprocessor\n",
    "\n",
    "    @cached_property\n",
    "    def medical_terms(self) -> list[str]:\n",
    "        # MeSH (Medical Subject Headings) is the NLM controlled\n",
    "        # vocabulary thesaurus used for indexing articles for PubMed.\n",
    "        # These terms list is generated using the terms extract for 2025 available in the site.\n",
    "        terms = []\n",
    "        with Path(\"./data/medical-terms-corpus.bin\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"MH = \"):\n",
    "                    term = line[5:].strip()\n",
    "                    terms.append(term.lower())\n",
    "        return terms\n",
    "\n",
    "    def add_document(self, document_path: Path):\n",
    "        document = self.__preprocessor.process_document(document_path=document_path)\n",
    "        self.__positional_index.add_document(document=document)\n",
    "\n",
    "    def documents(self):\n",
    "        return self.__positional_index.documents\n",
    "\n",
    "    def and_operator(self, left_posting_list: list[int], right_posting_list: list[int]) -> list[int]:\n",
    "        return sorted(set(left_posting_list).intersection(right_posting_list))\n",
    "\n",
    "    def or_operator(self, left_posting_list: list[int], right_posting_list: list[int]) -> list[int]:\n",
    "        return sorted(set(left_posting_list).union(right_posting_list))\n",
    "\n",
    "    def not_operator(self, posting_list: list[int]) -> list[int]:\n",
    "        return sorted(set(self.__positional_index.documents) - set(posting_list))\n",
    "\n",
    "    def optimize_query(self, terms: list[str]):\n",
    "        return sorted(\n",
    "            terms,\n",
    "            key=lambda term: self.__positional_index.get_term_frequency(term=term),\n",
    "        )\n",
    "\n",
    "    def execute_query(self, query: str, *, apply_spell_correction: bool = True):\n",
    "        original_query = query\n",
    "\n",
    "        if apply_spell_correction:\n",
    "            query = self.spelling_correct_query(query)\n",
    "            if query != original_query:\n",
    "                print(f\"Spelling correction applied: '{original_query}' -> '{query}'\")\n",
    "\n",
    "        terms = self.__preprocessor.process_text(query)\n",
    "        filtered_operator_keywords = [term for term in terms if term not in [\"and\", \"not\", \"or\"]]\n",
    "        optimized_order_terms = self.optimize_query(filtered_operator_keywords)\n",
    "        print(f\"Optimized processing order: {optimized_order_terms}\")\n",
    "\n",
    "        query = query.lower()\n",
    "        or_query_splits = query.split(\" or \")\n",
    "\n",
    "        and_not_results = [self.execute_and_not_query(query=or_query_split) for or_query_split in or_query_splits]\n",
    "\n",
    "        result = set()\n",
    "        for and_not_result in and_not_results:\n",
    "            result = self.or_operator(result, and_not_result)\n",
    "\n",
    "        return sorted(result)\n",
    "\n",
    "    def execute_and_not_query(self, query: str):\n",
    "        terms = self.__preprocessor.process_text(query)\n",
    "        # one main assumption made is we will get only straight forward queries\n",
    "        # That is after a keyword AND, OR, NOT there will always be a term\n",
    "        # NOT will always precede by AND or OR\n",
    "        # No nested queries in parentheses\n",
    "        # A term occurs only once in a query\n",
    "\n",
    "        result = set()\n",
    "        processed_query_term_indices = []\n",
    "\n",
    "        for index in range(len(terms)):\n",
    "            if index in processed_query_term_indices:\n",
    "                continue\n",
    "            if index == 0:\n",
    "                if terms[index] != \"not\":\n",
    "                    result.update(self.__positional_index.get_posting_list(term=terms[index]))\n",
    "                    processed_query_term_indices.append(index)\n",
    "                else:\n",
    "                    result.update(\n",
    "                        self.not_operator(self.__positional_index.get_posting_list(term=terms[index + 1])),\n",
    "                    )\n",
    "                    processed_query_term_indices.append(index)\n",
    "                    processed_query_term_indices.append(index + 1)\n",
    "            elif terms[index] == \"and\":\n",
    "                if terms[index + 1] != \"not\":\n",
    "                    result = self.and_operator(\n",
    "                        result,\n",
    "                        self.__positional_index.get_posting_list(term=terms[index + 1]),\n",
    "                    )\n",
    "                    processed_query_term_indices.append(index)\n",
    "                    processed_query_term_indices.append(index + 1)\n",
    "                else:\n",
    "                    not_result = self.not_operator(\n",
    "                        self.__positional_index.get_posting_list(term=terms[index + 2]),\n",
    "                    )\n",
    "                    result = self.and_operator(result, not_result)\n",
    "                    processed_query_term_indices.append(index)\n",
    "                    processed_query_term_indices.append(index + 1)\n",
    "                    processed_query_term_indices.append(index + 2)\n",
    "\n",
    "        return sorted(result)\n",
    "\n",
    "    def spelling_correction(self, term: str) -> str:\n",
    "        # If term exists in our vocabulary, return as is\n",
    "        processed_terms = self.__preprocessor.process_text(term)\n",
    "        if processed_terms and processed_terms[0] in self.__positional_index:\n",
    "            return term\n",
    "\n",
    "        # Check against medical terminology\n",
    "        if processed_terms:\n",
    "            query_term = processed_terms[0]\n",
    "\n",
    "            # Find closest term in our index\n",
    "            index_matches = difflib.get_close_matches(\n",
    "                query_term,\n",
    "                self.__positional_index.terms,\n",
    "                n=1,\n",
    "                cutoff=0.6,\n",
    "            )\n",
    "\n",
    "            if index_matches:\n",
    "                return index_matches[0]\n",
    "\n",
    "            # Find closest medical term\n",
    "            medical_matches = difflib.get_close_matches(\n",
    "                query_term,\n",
    "                self.medical_terms,\n",
    "                n=1,\n",
    "                cutoff=0.6,\n",
    "            )\n",
    "\n",
    "            if medical_matches:\n",
    "                return medical_matches[0]\n",
    "\n",
    "        return term  # Return original if no correction found\n",
    "\n",
    "    def spelling_correct_query(self, query: str) -> str:\n",
    "        tokens = query.split()\n",
    "        corrected_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.lower() in [\"and\", \"or\", \"not\"]:\n",
    "                corrected_tokens.append(token)\n",
    "            else:\n",
    "                corrected_tokens.append(self.spelling_correction(token))\n",
    "\n",
    "        return \" \".join(corrected_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "54c097d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "medical_index = PositionalInvertedIndex()\n",
    "\n",
    "search_engine = MedicalSearchEngine(positional_index=medical_index, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2c75d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document_path in Path(\"./data/medical-documents/\").iterdir():\n",
    "    search_engine.add_document(document_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "14c7c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling correction applied: 'cvid-19 and vacine or not dischare' -> 'covid19 and vaccine or not discharge'\n",
      "Optimized processing order: ['discharge', 'covid19', 'vaccine']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine.execute_query(\"cvid-19 and vacine or not dischare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2920d332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = set()\n",
    "x.update([1, 2, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b68cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f736087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
